{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Membership Inference Attacks in TAPAS\n",
    "\n",
    "Exploring the `TAPAS` library with the generator `DP-CGAN`\n",
    "\n",
    "Parameters are chosen to have the notebook run quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "\n",
    "import tapas.datasets\n",
    "import tapas.generators\n",
    "import tapas.threat_models\n",
    "import tapas.attacks\n",
    "import tapas.report\n",
    "\n",
    "from src.helpers import load_generator, load_tabular_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../datasets\"\n",
    "dataset_name = \"Adult\"\n",
    "model_name = \"DPCGANS\"\n",
    "file = f\"Real/real_{dataset_name.lower()}_data.csv\"\n",
    "\n",
    "schema = \"data_schemas/adult.json\"\n",
    "executable_generator = \"src/generator_dp_cgans.py\"\n",
    "\n",
    "\n",
    "# make some restrictions to speed up training\n",
    "N_subsample = 500 \n",
    "# keep only these columns for faster training. Needs to keep columns in the same order\n",
    "# columns_to_keep = [\"age\", \"education\", \"marital-status\", \"occupation\", \"race\", \"sex\", \"label\"]\n",
    "columns_to_keep = [\"age\", \"occupation\", \"race\", \"label\"] # this is much faster than the above\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "- I created a json file with the data schema in `data_schemas/adult.json`. I am not sure about all the \"countable\" data types, but none of the input data seems continuous/to have decimals. See the tapas documentation: https://privacy-sdg-toolbox.readthedocs.io/en/latest/dataset-schema.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_tabular_dataset(\n",
    "    filename_data=os.path.join(datapath, dataset_name, file),\n",
    "    filename_schema=schema,\n",
    "    N_subsample=N_subsample,\n",
    "    columns_to_keep=columns_to_keep\n",
    ")\n",
    "\n",
    "generator = load_generator(executable_generator, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the threat model\n",
    "\n",
    "We need to define \n",
    "- The membership of which record to attack\n",
    "- What data does the adversary have? Ie, which and how many records, how many samples do they have access to? \n",
    "- What does the adversary know about the synthetic data generator?\n",
    "\n",
    "This will all be collected in a `threat_model` object. We are then able to use the same threat model for different attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to separate the target record from the original data. We will generate synthetic datasets based on training data that either do or do not include the target record. The attacks then try to distinguish between the two classes of synthetic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>60</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>White</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age       occupation   race  label\n",
       "12491   60  Protective-serv  White  <=50K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attack_ids = [0]\n",
    "target_record = data.get_records(attack_ids)\n",
    "display(target_record.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_records(attack_ids, in_place=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the sample sizes \n",
    "- `n_reocrds_training`: number of records from the true data distribution that the adversary has available \n",
    "- `n_records_synth`: number of records in the generated synthetic data sets \n",
    "- In a real use case, this can be 1000s or many more, depending on the application/scenario we have in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records_training = 10\n",
    "n_records_synth = 10 \n",
    "\n",
    "# because we will use the ExactKnowledge scenario, we provide the exact data set of the size `n_records_training`. \n",
    "# If we used `AuxiliaryDataKnowledge`, we could specify these things in the definition of the threat model\n",
    "specific_data = data.sample(n_samples=n_records_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the threat model. We need to specify:\n",
    "1. data knowledge: the attacker knows the data set used to for the generator, but is only unsure about whether the target record was included or not (the two candidate data sets differ by one row). This directly links to the definition of differential privacy. How does this impact a given attack? \n",
    "    - Both from the definition context in the `TargetedMIA` and the class definition of `ExactDataKnowledge`, `generate_datasets` is the main method from the `attacker_knowledge` attribute. The method just returns `num_samples` copies of the training dataset (which excludes the target record)\n",
    "    - the method (`generate_samples??`) then uses those records, and randomly adds the target record to the training data set\n",
    "Alternative: auxiliaryknowledge. in this case, `generate_datasets` is more complicated\n",
    "    - split into aux data and test data -- what are they used for?\n",
    "    - then further split the aux or test data set into subsamples as specified \n",
    "    - note that we cannot reuse the generated synthetic data set when we change the assumption on data knowledge\n",
    "2. black-box knowledge on generator: the attacker can call the generator and create new synthetic datasets from a given input dataset (defined in data knowledge)\n",
    "\n",
    "#### Notes\n",
    "\n",
    "This attacker knowledge is interesting, because its modularity allows us to represent different scenarios of data sharing. (The thought experiment should be that assuming we released a synthetic dataset from a generator, how well do the data protect the privacy of the records in the dataset?)\n",
    "- Example: hospitals 1 and 2 provide their data (different records) to generate a synthetic dataset of cancer patients. hospital 1 knows that their records are used in the dataset. now the tought experiment is: given this knowledge, can someone in hospital 1 infer which patients from hospital 2 were included in the training set? in this scenario, the attacker knowledge is just exactly the data set of patients in hospital 1.\n",
    "- But what can we learn from this? Can we still conclude something about differential privacy with such a setting? \n",
    "\n",
    "\n",
    "Moreover\n",
    "- With a given threat model, we can run various attacks. \n",
    "- And it's possible (somehow, not exactly sure how) to re-use the synthetic datasets across attacks, and perhaps even across target records (open question) to be computationally efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_model = tapas.threat_models.TargetedMIA(\n",
    "    attacker_knowledge_data=tapas.threat_models.ExactDataKnowledge(\n",
    "        specific_data),       \n",
    "    attacker_knowledge_generator=tapas.threat_models.BlackBoxKnowledge(\n",
    "            generator, num_synthetic_records=n_records_synth,\n",
    "        ),\n",
    "    target_record=target_record,\n",
    "    generate_pairs=False, # TODO: what does this do exactly?\n",
    "    replace_target=False # TODO: what does this do exactly?\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12491</th>\n",
       "      <td>60</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>White</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age       occupation   race  label\n",
       "12491   60  Protective-serv  White  <=50K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(threat_model.num_labels)\n",
    "display(threat_model.target_record.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's noteworthy to track the `_memory` attribute. It is a dictionary with keys `True` and `False`, referring to training yes/no.\n",
    "Currently, the dict is empty; since `memorise_datasets` is true, the dict will be consecutively populated by synthetic datasets generated from the data knowledge in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atk_know_data': <tapas.threat_models.mia.MIALabeller at 0x7fdf94374490>,\n",
       " 'atk_know_gen': <tapas.threat_models.attacker_knowledge.BlackBoxKnowledge at 0x7fdf94374610>,\n",
       " 'memorise_datasets': True,\n",
       " 'iterator_tracker': tapas.threat_models.attacker_knowledge.SilentIterator,\n",
       " '_memory': {True: ([], []), False: ([], [])},\n",
       " 'num_labels': 1,\n",
       " 'num_concurrent': 1,\n",
       " 'multiple_label_mode': False,\n",
       " 'target_record': <tapas.datasets.dataset.TabularRecord at 0x7fdfad873910>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(threat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an attack\n",
    "\n",
    "Now we need to define the exact method the adversary uses to distinguish datasets that were generated with the target record included and not included. Many options are possible, and it is important to provide substantive reasoning why a given attack is not considered or why it is favored, relative to others. This depends also on the specific application and on our level of \"conservatism\" (ie, which criterion to use in the closest distance attack). \n",
    "\n",
    "Let's use the `ClosestDistance` attack with the standard distance (Hamming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = tapas.attacks.ClosestDistanceMIA(criterion=\"accuracy\", label=\"Closest-Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_criterion': 'accuracy',\n",
       " 'positive_label': None,\n",
       " 'negative_label': None,\n",
       " '_threshold': None,\n",
       " 'distance': <tapas.attacks.distances.HammingDistance at 0x7fdf950c3b20>,\n",
       " '_label': 'Closest-Distance'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to define a few more parameters\n",
    "\n",
    "What do they mean? (in both cases, 100 seems like a good number for a real check)\n",
    "- `n_training_datasets`: Number of data sets the adversary uses to train the attack\n",
    "- `n_testing_datasets`: Number of data sets to evaluate the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_datasets = 10\n",
    "n_testing_datasets = 12 # to highlight the distinction below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the attack\n",
    "\n",
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack.train(threat_model, num_samples=n_training_datasets) # that's short, 55.2 seconds with few columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect now the threat model\n",
    "- `._memory[True]` now has `n_training_datasets` of datasets, but none in `._memory[False]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atk_know_data': <tapas.threat_models.mia.MIALabeller at 0x7fdf94374490>,\n",
       " 'atk_know_gen': <tapas.threat_models.attacker_knowledge.BlackBoxKnowledge at 0x7fdf94374610>,\n",
       " 'memorise_datasets': True,\n",
       " 'iterator_tracker': tapas.threat_models.attacker_knowledge.SilentIterator,\n",
       " '_memory': {True: ([<tapas.datasets.dataset.TabularDataset at 0x7fdfe83aacd0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfe83aab80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9e80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9ee0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9460>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9520>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5b80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5e80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5550>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5340>],\n",
       "   [True, False, True, False, True, False, False, True, False, False]),\n",
       "  False: ([], [])},\n",
       " 'num_labels': 1,\n",
       " 'num_concurrent': 1,\n",
       " 'multiple_label_mode': False,\n",
       " 'target_record': <tapas.datasets.dataset.TabularRecord at 0x7fdfad873910>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(attack.threat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_summary = threat_model.test(attack, num_samples=n_testing_datasets) # 100 takes 10 minutes. 10 takes 40 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are `n_testing_datasets` datasets in `._memory[False]`. They are generated during testing the attack above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'atk_know_data': <tapas.threat_models.mia.MIALabeller at 0x7fdf94374490>,\n",
       " 'atk_know_gen': <tapas.threat_models.attacker_knowledge.BlackBoxKnowledge at 0x7fdf94374610>,\n",
       " 'memorise_datasets': True,\n",
       " 'iterator_tracker': tapas.threat_models.attacker_knowledge.SilentIterator,\n",
       " '_memory': {True: ([<tapas.datasets.dataset.TabularDataset at 0x7fdfe83aacd0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfe83aab80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9e80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9ee0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9460>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf943a9520>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5b80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5e80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5550>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94c5340>],\n",
       "   [True, False, True, False, True, False, False, True, False, False]),\n",
       "  False: ([<tapas.datasets.dataset.TabularDataset at 0x7fdf9438dfd0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdf9438ddc0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94ca910>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94caeb0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94dbac0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94dbdf0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94dbb80>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94db0a0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94db7f0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94deaf0>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94dee20>,\n",
       "    <tapas.datasets.dataset.TabularDataset at 0x7fdfc94dea30>],\n",
       "   [False,\n",
       "    True,\n",
       "    False,\n",
       "    False,\n",
       "    False,\n",
       "    False,\n",
       "    True,\n",
       "    False,\n",
       "    True,\n",
       "    False,\n",
       "    True,\n",
       "    False])},\n",
       " 'num_labels': 1,\n",
       " 'num_concurrent': 1,\n",
       " 'multiple_label_mode': False,\n",
       " 'target_record': <tapas.datasets.dataset.TabularRecord at 0x7fdfad873910>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(attack.threat_model) # so now `._memory[False]` also has `num_samples` datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More notes\n",
    "- When a new attack is run on the same threat model (and `memorise_datasets` is `True`), then any pre-existing training/test data sets are reused for future attacks. \n",
    "- If an attack requires more datasets than stored for either training or testing, the number of missing datasets is generated by again calling the generator\n",
    "- An implication of this is there is a \"convienent parallelism\" across threat models, but not across attacks for the same threat model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the results\n",
    "\n",
    "This is not meaningful with the small samples we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -2., -2., -2., -1., -1., -2., -1., -1., -1., -2.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(attack_summary.scores) # what do the scores mean? try with smaller samples?\n",
    "display(len(attack_summary.scores)) # these are the number of samples in the test()\n",
    "display(attack_summary.labels) # I guess these are the indicators for whether the dataset contains the record or not?\n",
    "\n",
    "\n",
    "type(attack_summary)\n",
    "attack_summary.predictions # so this explains why the FPR and TPR are 0. How can I change it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = attack_summary.get_metrics() # but then I get again this RuntimeWarning: invalid value encountered in divide\n",
    "#   return np.log(max(np.max(tp / fp), np.max((1 - fp) / (1 - tp)))) -- but the TP/FP below are not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>target_id</th>\n",
       "      <th>generator</th>\n",
       "      <th>attack</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>true_positive_rate</th>\n",
       "      <th>false_positive_rate</th>\n",
       "      <th>mia_advantage</th>\n",
       "      <th>privacy_gain</th>\n",
       "      <th>auc</th>\n",
       "      <th>effective_epsilon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed dataset (EXACT)</td>\n",
       "      <td>0</td>\n",
       "      <td>src/generator_dp_cgans.py</td>\n",
       "      <td>Closest-Distance</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dataset target_id                  generator  \\\n",
       "0  Unnamed dataset (EXACT)         0  src/generator_dp_cgans.py   \n",
       "\n",
       "             attack  accuracy  true_positive_rate  false_positive_rate  \\\n",
       "0  Closest-Distance      0.75                 1.0                0.375   \n",
       "\n",
       "   mia_advantage  privacy_gain     auc  effective_epsilon  \n",
       "0          0.625         0.375  0.8125                inf  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Open questions \n",
    "- When using auxiliary data knowledge, where/how are exactly are `aux_data` and `test_data` used? I could not immediately see it in the code\n",
    "- How does it work with multiple target records?\n",
    "\n",
    "**Multiple target records**\n",
    "\n",
    "I don't understand how this works. \n",
    "\n",
    "We can pass more than one targets in `target_record`. While the resulting object's target records is always the first record, the `threat_model` object has an attribute `_target_records` that stores all the provided targets. I guess the idea is that we can re-use the same target model on the same target record (with the same synthetic data sets generated?).\n",
    "\n",
    "\n",
    "In other words, all the `_target_records` are excluded from the data set that is passed to the synthetic data generator. For an attack, we then use one of the `_target_records` and add it randomly to some of the training datasets that are used to train the attack. (Recall that training dataset = one synthetic dataset plus, by chance, the target record or not).\n",
    "This is handy because we don't have to call the generator multiple times for using the same threat model for multiple records. But, if my understanding above is correct, this can also bias the generated synthetic data set because all of the target records are excluded from the training data to the generator. (\n",
    "    \n",
    "But I am wondering whether is this statistically proper? Ie, could it lead to bias in the sense that depending on which *other* target records $x_1$, $x_2$, ..., the audit results for target record $x_0$ differ when we drop other target records from the training data and when we don't? under which assumptions? under which sample size?\n",
    "\n",
    "The documentation says something about this (it's related to the parameters `replace_target` and `generate_pairs` above, so check those out).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
